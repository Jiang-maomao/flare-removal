{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "#from basicsr.data.flare7k_dataset import Flare_Image_Loader,RandomGammaCorrection\n",
    "from basicsr.archs.uformer_arch import Uformer\n",
    "#from basicsr.archs.unet_arch import U_Net\n",
    "from basicsr.utils.flare_util import blend_light_source,get_args_from_json,save_args_to_json,mkdir,predict_flare_from_6_channel,predict_flare_from_3_channel\n",
    "from torch.distributions import Normal\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "from models import *\n",
    "from collections import OrderedDict\n",
    "\n",
    "def mkdir(path):\n",
    "\tfolder = os.path.exists(path)\n",
    "\tif not folder:\n",
    "\t\tos.makedirs(path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, weights):\n",
    "    checkpoint = torch.load(\n",
    "        weights, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, value in checkpoint['state_dict'].items():\n",
    "        if key.startswith('module'):\n",
    "            name = key[7:]\n",
    "        else:\n",
    "            name = key\n",
    "        new_state_dict[name] = value\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "def demo_real(images_path,output_path,model_type,output_ch,pretrain_dir):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    test_path=sorted(glob.glob(images_path))\n",
    "    result_path=output_path\n",
    "    torch.cuda.empty_cache()\n",
    "    if model_type=='Uformer':\n",
    "        model=Uformer(img_size=512,img_ch=3,output_ch=output_ch).cuda()\n",
    "        model.load_state_dict(torch.load(pretrain_dir))\n",
    "    elif model_type=='U_Net' or model_type=='U-Net':\n",
    "        model=U_Net(img_ch=3,output_ch=output_ch).cuda()\n",
    "        model.load_state_dict(torch.load(pretrain_dir))\n",
    "    elif model_type=='ours':\n",
    "        model=Model().cuda()\n",
    "        load_checkpoint(model,pretrain_dir)\n",
    "    else:\n",
    "        assert False, \"This model is not supported!!\"\n",
    "    to_tensor=transforms.ToTensor()\n",
    "    resize=transforms.Resize((512,512)) #The output should in the shape of 128X\n",
    "    for i,image_path in tqdm(enumerate(test_path)):\n",
    "        mkdir(result_path+\"deflare/\")\n",
    "        mkdir(result_path+\"flare/\")\n",
    "        mkdir(result_path+\"input/\")\n",
    "        mkdir(result_path+\"blend/\")\n",
    "\n",
    "        deflare_path = result_path+\"deflare/\"+str(i).zfill(5)+\"_deflare.png\"\n",
    "        flare_path = result_path+\"flare/\"+str(i).zfill(5)+\"_flare.png\"\n",
    "        merge_path = result_path+\"input/\"+str(i).zfill(5)+\"_input.png\"\n",
    "        blend_path = result_path+\"blend/\"+str(i).zfill(5)+\"_blend.png\"\n",
    "\n",
    "        merge_img = Image.open(image_path).convert(\"RGB\")\n",
    "        #merge_img = resize(to_tensor(merge_img))\n",
    "        merge_img = (to_tensor(merge_img)).cuda().unsqueeze(0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output_img=model(merge_img)\n",
    "            print(output_img.shape)\n",
    "            #if ch is 6, first three channels are deflare image, others are flare image\n",
    "            #if ch is 3, unsaturated region of output is the deflare image.\n",
    "            gamma=torch.Tensor([2.2])\n",
    "            if output_ch==6:\n",
    "                deflare_img,flare_img_predicted,merge_img_predicted=predict_flare_from_6_channel(output_img,gamma)\n",
    "            elif output_ch==3:\n",
    "                flare_mask=torch.zeros_like(merge_img)\n",
    "                deflare_img,flare_img_predicted=predict_flare_from_3_channel(output_img,flare_mask,output_img,merge_img,merge_img,gamma)\n",
    "            else:\n",
    "                assert False, \"This output_ch is not supported!!\"\n",
    "\n",
    "            blend_img= blend_light_source(merge_img, deflare_img, 0.945)\n",
    "\n",
    "            torchvision.utils.save_image(merge_img, merge_path)\n",
    "            torchvision.utils.save_image(flare_img_predicted, flare_path)\n",
    "            torchvision.utils.save_image(deflare_img, deflare_path)\n",
    "            torchvision.utils.save_image(blend_img, blend_path)\n",
    "\n",
    "\n",
    "def demo_syn(images_path, output_path, model_type, output_ch, pretrain_dir):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    test_path = sorted(glob.glob(images_path))\n",
    "    result_path = output_path\n",
    "    torch.cuda.empty_cache()\n",
    "    if model_type == 'Uformer':\n",
    "        model = Uformer(img_size=512, img_ch=3, output_ch=output_ch).cuda()\n",
    "        model.load_state_dict(torch.load(pretrain_dir))\n",
    "    elif model_type == 'U_Net' or model_type == 'U-Net':\n",
    "        model = U_Net(img_ch=3, output_ch=output_ch).cuda()\n",
    "        model.load_state_dict(torch.load(pretrain_dir))\n",
    "    elif model_type == 'ours':\n",
    "        model = Model().cuda()\n",
    "        load_checkpoint(model, pretrain_dir)\n",
    "    else:\n",
    "        assert False, \"This model is not supported!!\"\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    # The output should in the shape of 128X\n",
    "    resize = transforms.Resize((512, 512))\n",
    "    for i, image_path in tqdm(enumerate(test_path)):\n",
    "        mkdir(result_path+\"deflare/\")\n",
    "        mkdir(result_path+\"flare/\")\n",
    "        mkdir(result_path+\"input/\")\n",
    "        mkdir(result_path+\"blend/\")\n",
    "\n",
    "        deflare_path = result_path+\"deflare/\"+str(i).zfill(5)+\"_deflare.png\"\n",
    "        flare_path = result_path+\"flare/\"+str(i).zfill(5)+\"_flare.png\"\n",
    "        merge_path = result_path+\"input/\"+str(i).zfill(5)+\"_input.png\"\n",
    "        blend_path = result_path+\"blend/\"+str(i).zfill(5)+\"_blend.png\"\n",
    "\n",
    "        merge_img = Image.open(image_path).convert(\"RGB\")\n",
    "        #merge_img = resize(to_tensor(merge_img))\n",
    "        merge_img = (to_tensor(merge_img)).cuda().unsqueeze(0)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output_img = model(merge_img)\n",
    "            print(output_img.shape)\n",
    "            #if ch is 6, first three channels are deflare image, others are flare image\n",
    "            #if ch is 3, unsaturated region of output is the deflare image.\n",
    "            gamma = torch.Tensor([2.2])\n",
    "            if output_ch == 6:\n",
    "                deflare_img, flare_img_predicted, merge_img_predicted = predict_flare_from_6_channel(\n",
    "                    output_img, gamma)\n",
    "            elif output_ch == 3:\n",
    "                flare_mask = torch.zeros_like(merge_img)\n",
    "                deflare_img, flare_img_predicted = predict_flare_from_3_channel(\n",
    "                    output_img, flare_mask, output_img, merge_img, merge_img, gamma)\n",
    "            else:\n",
    "                assert False, \"This output_ch is not supported!!\"\n",
    "\n",
    "            blend_img = blend_light_source(merge_img, deflare_img, 0.999)\n",
    "\n",
    "            torchvision.utils.save_image(merge_img, merge_path)\n",
    "            torchvision.utils.save_image(flare_img_predicted, flare_path)\n",
    "            torchvision.utils.save_image(deflare_img, deflare_path)\n",
    "            torchvision.utils.save_image(blend_img, blend_path)\n",
    "\n",
    "\n",
    "#model_type=\"Uformer\"\n",
    "model_type=\"ours\"\n",
    "images_path_real=\"datasets/Flare7k/test_data/real/input/*.*\"\n",
    "images_path_syn = \"datasets/Flare7k/test_data/synthetic/input/*.*\"\n",
    "#images_path = \"datasets/4ks/*.*\"\n",
    "\n",
    "\n",
    "#result_path=\"result/real/Uformer_noreflection/\"\n",
    "#result_path = \"result/synthetic/Uformer/\"\n",
    "result_path_syn = \"\"\n",
    "result_path_real = \"\"\n",
    "#result_path=\"result/4ks/\"\n",
    "#pretrain_dir='experiments/pretrained_models/uformer/net_g_last.pth'\n",
    "pretrain_dir = ''\n",
    "output_ch=3\n",
    "mask_flag=False\n",
    "\n",
    "\n",
    "\n",
    "demo_real(images_path_real, result_path_real, model_type, output_ch, pretrain_dir)\n",
    "demo_syn(images_path_syn, result_path_syn,\n",
    "          model_type, output_ch, pretrain_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b70b297bdda78e1dcf196f39c503f752b358c81b8cc936fc839b5c3b43f85b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3d435e51e702e7b0995eba7d152ca2d2b0b33732f1590a6d012c8aa76dc59cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
